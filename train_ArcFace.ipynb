{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ArcFace Training\n",
    "\n",
    "This notebook builds and trains an ArcFace based deep neural network.\n",
    "\n",
    "References: \n",
    "1. https://www.kaggle.com/ragnar123/shopee-efficientnetb3-arcmarginproduct/notebook\n",
    "2. https://www.kaggle.com/ragnar123/shopee-tf-records-512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import EfficientNetB3\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import backend as K\n",
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfrecords directory\n",
    "TFRECORDS = tf.io.gfile.glob('../../data/tfrecords-new/*.tfrec')\n",
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 8\n",
    "IMAGE_SIZE = [512, 512]\n",
    "SEED = 42\n",
    "LR = 0.001\n",
    "N_CLASSES = 11014\n",
    "\n",
    "AUTO = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    '''\n",
    "    Seed for reproducibility\n",
    "    '''\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arcface_format(posting_id, image, label_group, matches):\n",
    "    '''\n",
    "    Transforms our dataset to the ArcFace input format\n",
    "    '''\n",
    "    return posting_id, {'inp1': image, 'inp2': label_group}, label_group, matches\n",
    "\n",
    "def data_augment(posting_id, image, label_group, matches):\n",
    "    '''\n",
    "    image augmentation\n",
    "    '''\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_flip_up_down(image)\n",
    "    image = tf.image.random_hue(image, 0.01)\n",
    "    image = tf.image.random_saturation(image, 0.70, 1.30)\n",
    "    image = tf.image.random_contrast(image, 0.80, 1.20)\n",
    "    image = tf.image.random_brightness(image, 0.10)\n",
    "    return posting_id, image, label_group, matches\n",
    "\n",
    "\n",
    "def read_tfrecord(example, labeled=True):\n",
    "    '''\n",
    "    Parses a single image from tfrecords\n",
    "    '''\n",
    "    tfrecord_format = {\n",
    "        \"posting_id\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"label_group\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"matches\": tf.io.FixedLenFeature([], tf.string)\n",
    "    }\n",
    "\n",
    "    example = tf.io.parse_single_example(example, tfrecord_format)\n",
    "    posting_id = example['posting_id']\n",
    "    image = util.decode_image(example['image'],IMAGE_SIZE)\n",
    "    if labeled:\n",
    "        label_group = tf.cast(example['label_group'], tf.int32)\n",
    "    matches = example['matches']\n",
    "    return posting_id, image, label_group, matches\n",
    "\n",
    "def load_dataset(filenames, ordered = False):\n",
    "    '''\n",
    "    Load tfrecords and parse into a tf.data.TFRecordDataset\n",
    "    '''\n",
    "    ignore_order = tf.data.Options()\n",
    "    # disable order, increase speed\n",
    "    if not ordered: ignore_order.experimental_deterministic = False \n",
    "        \n",
    "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads = AUTO)\n",
    "    # automatically interleaves reads from multiple files\n",
    "    dataset = dataset.with_options(ignore_order)\n",
    "    dataset = dataset.map(read_tfrecord, num_parallel_calls = AUTO) \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_dataset(filenames, ordered = False):\n",
    "    dataset = load_dataset(filenames, ordered = ordered)\n",
    "    dataset = dataset.map(data_augment, num_parallel_calls = AUTO)\n",
    "    dataset = dataset.map(arcface_format, num_parallel_calls = AUTO)\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.shuffle(2048)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(AUTO)\n",
    "    return dataset\n",
    "\n",
    "def get_validation_dataset(filenames, ordered = True):\n",
    "    dataset = load_dataset(filenames, ordered = ordered)\n",
    "    dataset = dataset.map(arcface_format, num_parallel_calls = AUTO)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(AUTO) \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_data_items(filenames):\n",
    "    '''\n",
    "    Parses filenames of tfrecords to get number of images\n",
    "    '''\n",
    "    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n",
    "    return np.sum(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr_callback():\n",
    "    '''\n",
    "    Custom learning rate scheduler\n",
    "    '''\n",
    "    \n",
    "    lr_start   = 0.000001\n",
    "    lr_max     = 0.000005 * BATCH_SIZE\n",
    "    lr_min     = 0.000001\n",
    "    lr_ramp_ep = 5\n",
    "    lr_sus_ep  = 0\n",
    "    lr_decay   = 0.8\n",
    "   \n",
    "    def lrfn(epoch):\n",
    "        if epoch < lr_ramp_ep:\n",
    "            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start   \n",
    "        elif epoch < lr_ramp_ep + lr_sus_ep:\n",
    "            lr = lr_max    \n",
    "        else:\n",
    "            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min    \n",
    "        return lr\n",
    "\n",
    "    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)\n",
    "    return lr_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "\n",
    "    inp = tf.keras.layers.Input(shape = (*IMAGE_SIZE, 3), name = 'inp1')\n",
    "    label = tf.keras.layers.Input(shape = (), name = 'inp2')\n",
    "    x = EfficientNetB3(weights = 'imagenet', include_top = False)(inp)\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    margin = util.ArcMarginProduct(\n",
    "        n_classes = N_CLASSES, \n",
    "        s = 30, \n",
    "        m = 0.5, \n",
    "        name='head/arc_margin', \n",
    "        dtype='float32'\n",
    "    )\n",
    "    \n",
    "    x = margin([x, label])\n",
    "\n",
    "    output = tf.keras.layers.Softmax(dtype='float32')(x)\n",
    "    model = tf.keras.models.Model(inputs = [inp, label], outputs = [output])\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate = LR)\n",
    "    model.compile(\n",
    "        optimizer = opt,\n",
    "        loss = ['sparse_categorical_crossentropy'],\n",
    "        metrics = ['sparse_categorical_accuracy']\n",
    "        ) \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid = train_test_split(TFRECORDS, shuffle = True, random_state = SEED, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = get_training_dataset(train, ordered = False)\n",
    "train_dataset = train_dataset.map(lambda posting_id, image, label_group, matches: (image, label_group))\n",
    "val_dataset = get_validation_dataset(valid, ordered = True)\n",
    "val_dataset = val_dataset.map(lambda posting_id, image, label_group, matches: (image, label_group))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(f'EfficientNetB3_{IMAGE_SIZE[0]}_{SEED}_new_lr.h5', \n",
    "                                                monitor = 'val_loss', \n",
    "                                                verbose = 1, \n",
    "                                                save_best_only = True,\n",
    "                                                save_weights_only = True, \n",
    "                                                mode = 'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('./trained/arcface_best_epoch_512_42.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEPS_PER_EPOCH = count_data_items(train) // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 1e-06.\n",
      "3710/3710 [==============================] - 2060s 534ms/step - loss: 2.6169 - sparse_categorical_accuracy: 0.6630 - val_loss: 9.8491 - val_sparse_categorical_accuracy: 0.3458\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 9.84911, saving model to EfficientNetB5_512_42_new_lr.h5\n",
      "Epoch 2/20\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 8.800000000000002e-06.\n",
      "3710/3710 [==============================] - 1976s 533ms/step - loss: 2.6734 - sparse_categorical_accuracy: 0.6601 - val_loss: 9.9598 - val_sparse_categorical_accuracy: 0.3384\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 9.84911\n",
      "Epoch 3/20\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 1.6600000000000004e-05.\n",
      "3710/3710 [==============================] - 1977s 533ms/step - loss: 2.6555 - sparse_categorical_accuracy: 0.6615 - val_loss: 10.3044 - val_sparse_categorical_accuracy: 0.3143\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 9.84911\n",
      "Epoch 4/20\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 2.4400000000000007e-05.\n",
      "3710/3710 [==============================] - 1975s 532ms/step - loss: 2.6260 - sparse_categorical_accuracy: 0.6659 - val_loss: 13.6312 - val_sparse_categorical_accuracy: 0.0749\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 9.84911\n",
      "Epoch 5/20\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 3.2200000000000003e-05.\n",
      "3710/3710 [==============================] - 1969s 531ms/step - loss: 2.4710 - sparse_categorical_accuracy: 0.6850 - val_loss: 14.9903 - val_sparse_categorical_accuracy: 0.0346\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 9.84911\n",
      "Epoch 6/20\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 4e-05.\n",
      "3710/3710 [==============================] - 1977s 533ms/step - loss: 2.3367 - sparse_categorical_accuracy: 0.7061 - val_loss: 13.1447 - val_sparse_categorical_accuracy: 0.0924\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 9.84911\n",
      "Epoch 7/20\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 3.2200000000000003e-05.\n",
      "3710/3710 [==============================] - 1976s 533ms/step - loss: 2.0037 - sparse_categorical_accuracy: 0.7539 - val_loss: 9.8599 - val_sparse_categorical_accuracy: 0.3482\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 9.84911\n",
      "Epoch 8/20\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 2.596000000000001e-05.\n",
      "3710/3710 [==============================] - 1969s 531ms/step - loss: 1.7158 - sparse_categorical_accuracy: 0.7977 - val_loss: 10.3382 - val_sparse_categorical_accuracy: 0.2976\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 9.84911\n",
      "Epoch 9/20\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 2.096800000000001e-05.\n",
      "3710/3710 [==============================] - 1977s 533ms/step - loss: 1.5388 - sparse_categorical_accuracy: 0.8256 - val_loss: 10.3919 - val_sparse_categorical_accuracy: 0.2957\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 9.84911\n",
      "Epoch 10/20\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 1.6974400000000005e-05.\n",
      "3710/3710 [==============================] - 1976s 533ms/step - loss: 1.3729 - sparse_categorical_accuracy: 0.8514 - val_loss: 9.3533 - val_sparse_categorical_accuracy: 0.3999\n",
      "\n",
      "Epoch 00010: val_loss improved from 9.84911 to 9.35333, saving model to EfficientNetB5_512_42_new_lr.h5\n",
      "Epoch 11/20\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 1.3779520000000006e-05.\n",
      "3710/3710 [==============================] - 1976s 533ms/step - loss: 1.2528 - sparse_categorical_accuracy: 0.8697 - val_loss: 9.0479 - val_sparse_categorical_accuracy: 0.4319\n",
      "\n",
      "Epoch 00011: val_loss improved from 9.35333 to 9.04791, saving model to EfficientNetB5_512_42_new_lr.h5\n",
      "Epoch 12/20\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 1.1223616000000006e-05.\n",
      "3710/3710 [==============================] - 1977s 533ms/step - loss: 1.1697 - sparse_categorical_accuracy: 0.8823 - val_loss: 8.8999 - val_sparse_categorical_accuracy: 0.4452\n",
      "\n",
      "Epoch 00012: val_loss improved from 9.04791 to 8.89989, saving model to EfficientNetB5_512_42_new_lr.h5\n",
      "Epoch 13/20\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 9.178892800000006e-06.\n",
      "3710/3710 [==============================] - 1976s 533ms/step - loss: 1.1034 - sparse_categorical_accuracy: 0.8916 - val_loss: 9.1798 - val_sparse_categorical_accuracy: 0.4187\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 8.89989\n",
      "Epoch 14/20\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 7.543114240000004e-06.\n",
      "3710/3710 [==============================] - 1958s 528ms/step - loss: 1.0416 - sparse_categorical_accuracy: 0.8978 - val_loss: 9.1007 - val_sparse_categorical_accuracy: 0.4282\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 8.89989\n",
      "Epoch 15/20\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 6.234491392000003e-06.\n",
      "3710/3710 [==============================] - 1978s 533ms/step - loss: 0.9932 - sparse_categorical_accuracy: 0.9059 - val_loss: 8.8149 - val_sparse_categorical_accuracy: 0.4518\n",
      "\n",
      "Epoch 00015: val_loss improved from 8.89989 to 8.81491, saving model to EfficientNetB5_512_42_new_lr.h5\n",
      "Epoch 16/20\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 5.1875931136000026e-06.\n",
      "3710/3710 [==============================] - 1979s 533ms/step - loss: 0.9642 - sparse_categorical_accuracy: 0.9102 - val_loss: 8.8905 - val_sparse_categorical_accuracy: 0.4461\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 8.81491\n",
      "Epoch 17/20\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 4.3500744908800025e-06.\n",
      "3710/3710 [==============================] - 1989s 536ms/step - loss: 0.9251 - sparse_categorical_accuracy: 0.9136 - val_loss: 8.7388 - val_sparse_categorical_accuracy: 0.4569\n",
      "\n",
      "Epoch 00017: val_loss improved from 8.81491 to 8.73878, saving model to EfficientNetB5_512_42_new_lr.h5\n",
      "Epoch 18/20\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 3.6800595927040018e-06.\n",
      "3710/3710 [==============================] - 1975s 532ms/step - loss: 0.9087 - sparse_categorical_accuracy: 0.9169 - val_loss: 8.7533 - val_sparse_categorical_accuracy: 0.4560\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 8.73878\n",
      "Epoch 19/20\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 3.144047674163202e-06.\n",
      "3710/3710 [==============================] - 1976s 533ms/step - loss: 0.8803 - sparse_categorical_accuracy: 0.9208 - val_loss: 8.7221 - val_sparse_categorical_accuracy: 0.4604\n",
      "\n",
      "Epoch 00019: val_loss improved from 8.73878 to 8.72213, saving model to EfficientNetB5_512_42_new_lr.h5\n",
      "Epoch 20/20\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 2.7152381393305616e-06.\n",
      "3710/3710 [==============================] - 1978s 533ms/step - loss: 0.8726 - sparse_categorical_accuracy: 0.9222 - val_loss: 8.7287 - val_sparse_categorical_accuracy: 0.4593\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 8.72213\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_dataset,\n",
    "                    steps_per_epoch = STEPS_PER_EPOCH,\n",
    "                    epochs = EPOCHS,\n",
    "#                    callbacks = [checkpoint],\n",
    "                    callbacks = [checkpoint, get_lr_callback()], \n",
    "                    validation_data = val_dataset,\n",
    "                    verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/renyc/Anaconda/anaconda3/envs/python3.8.3/lib/python3.8/site-packages/tensorflow/python/keras/utils/generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n"
     ]
    }
   ],
   "source": [
    "model.save('./trained/arcface.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
